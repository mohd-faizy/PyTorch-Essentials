{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PyTorch Vs Numpy Speed Comparision**"
      ],
      "metadata": {
        "id": "FXLtf_2MYjbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import random\n",
        "\n",
        "mat_size = 10000\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# NumPy computation\n",
        "start_time_np = time.time()\n",
        "np_mat1 = np.random.rand(mat_size, mat_size)\n",
        "np_mat2 = np.random.rand(mat_size, mat_size)\n",
        "np_result = np.dot(np_mat1, np_mat2)\n",
        "end_time_np = time.time()\n",
        "np_processing_time = end_time_np - start_time_np\n",
        "print(f\"NumPy Processing time: {np_processing_time:.5f} sec\")\n",
        "\n",
        "# PyTorch computation\n",
        "start_time_torch = time.time()\n",
        "torch_mat1 = torch.rand(mat_size, mat_size, device=device)\n",
        "torch_mat2 = torch.rand(mat_size, mat_size, device=device)\n",
        "torch_result = torch.matmul(torch_mat1, torch_mat2)\n",
        "end_time_torch = time.time()\n",
        "torch_processing_time = end_time_torch - start_time_torch\n",
        "print(f\"PyTorch Processing time: {torch_processing_time:.5f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSd55NpTVp8b",
        "outputId": "41ca3b42-2327-4dff-fafd-3e471bca608a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy Processing time: 36.59360 sec\n",
            "PyTorch Processing time: 0.00058 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Speed comparison\n",
        "if torch_processing_time < np_processing_time:\n",
        "    speedup = np_processing_time / torch_processing_time\n",
        "    print(f\"✅ PyTorch is faster by {speedup:.2f}x\")\n",
        "else:\n",
        "    slowdown = torch_processing_time / np_processing_time\n",
        "    print(f\"⚠️ NumPy is faster by {slowdown:.2f}x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sascyh1aX52r",
        "outputId": "37485c18-5aec-467d-c485-40b395bbdef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PyTorch is faster by 62723.61x\n"
          ]
        }
      ]
    }
  ]
}