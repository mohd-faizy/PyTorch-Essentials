{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HZTaV9_m6wXH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ⭐**Dataset & Dataloader**"
      ],
      "metadata": {
        "id": "Sfxzw3n2Wrk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**"
      ],
      "metadata": {
        "id": "Gm8mgRgG8sbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅**The previous training pipeline used in the breast cancer classification code had a significant flaw:**\n",
        "\n",
        "- It used **Batch Gradient Descent**.\n",
        "\n",
        "- This approach involves using the **entire dataset** simultaneously for parameter updates.\n",
        "\n",
        "- It is very **memory inefficient** because the whole dataset needs to be loaded into RAM at once. This is impractical for large datasets like image classification problems with gigabytes of data.\n",
        "\n",
        "- It results in **slower convergence** because parameter updates occur less frequently (only once per epoch using the whole dataset), making the algorithm converge slowly to optimal parameter values.\n",
        "\n"
      ],
      "metadata": {
        "id": "klg4uL0zX6_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅**The cycle of this flawed approach involved:**\n",
        "\n",
        "- Using the **entire dataset** simultaneously for a single parameter update.\n",
        "- The **whole dataset** was sent to the forward pass.\n",
        "- `Loss` and `gradients` were calculated based on this **entire dataset**.\n",
        "- Parameter updates happened only **once per epoch**, using the gradients from the full dataset."
      ],
      "metadata": {
        "id": "FyQSzCuLZW_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅**Major problems with Batch Gradient Descent are:**\n",
        "\n",
        "\n",
        "- **Memory Inefficiency**: It requires loading the **entire dataset into RAM** simultaneously to perform parameter updates. This is highly impractical and often impossible for large datasets, such as those found in image classification with gigabytes of data.\n",
        "\n",
        "- **Slow Convergence**: Parameter updates happen only **once per epoch** after processing the entire dataset. These infrequent updates lead to the algorithm converging slowly to good parameter values."
      ],
      "metadata": {
        "id": "ml8m0NTMZsBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅**Mini-Batch Gradient Descent is an alternative to Batch Gradient Descent:**\n",
        "\n",
        "*   It **divides the entire dataset into smaller batches**.\n",
        "*   Training involves processing **one batch at a time**.\n",
        "*   For each batch, a **forward pass, loss calculation, and parameter update** occur before moving to the next batch.\n",
        "*   This method is **more memory efficient** and generally leads to **faster convergence** compared to Batch Gradient Descent."
      ],
      "metadata": {
        "id": "Q_3SXzk2b110"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The major problems with manual implementation of mini-batch gradient descent (before using PyTorch's Dataset and DataLoader) were:\n",
        "\n",
        "- **No Standard Data Interface:** Difficulty in loading data from various sources (like image folders) to create the tensors needed for batches. There's no clear, standard way to handle this.\n",
        "\n",
        "- **Applying Transformations is Difficult:** No easy place or standardized method to apply transformations (like resizing images or text processing) to data within batches before training.\n",
        "\n",
        "- **Shuffling and Sampling Issues:** Handling random shuffling and complex sampling strategies (like for imbalanced datasets) is not straightforward with manual batch creation.\n",
        "\n",
        "- **Inefficient Batch Management/No Parallelization:** Manually managing batches is sequential and doesn't easily allow for parallel data loading to speed up the process.\n",
        "\n",
        "> These issues are specifically what PyTorch's Dataset and DataLoader classes are designed to solve, making the implementation of mini-batch gradient descent robust and efficient"
      ],
      "metadata": {
        "id": "0pi2PrH7dI_5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TgAOwS2a3Keb"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create a synthetic classification dataset using sklearn\n",
        "X, y = make_classification(\n",
        "    n_samples=10,       # Number of samples\n",
        "    n_features=2,       # Number of features\n",
        "    n_informative=2,    # Number of informative features\n",
        "    n_redundant=0,      # Number of redundant features\n",
        "    n_classes=2,        # Number of classes\n",
        "    random_state=42     # For reproducibility\n",
        ")"
      ],
      "metadata": {
        "id": "aqEej4GC3UyT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WywhcnAj3Yec",
        "outputId": "a27744c3-15d6-4f91-d239-085aabe6c037"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.06833894, -0.97007347],\n",
              "       [-1.14021544, -0.83879234],\n",
              "       [-2.8953973 ,  1.97686236],\n",
              "       [-0.72063436, -0.96059253],\n",
              "       [-1.96287438, -0.99225135],\n",
              "       [-0.9382051 , -0.54304815],\n",
              "       [ 1.72725924, -1.18582677],\n",
              "       [ 1.77736657,  1.51157598],\n",
              "       [ 1.89969252,  0.83444483],\n",
              "       [-0.58723065, -1.97171753]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBb4gaS_3cYe",
        "outputId": "bd475a64-ab5d-42f9-8393-dae417ec6df9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAAejfi23fkA",
        "outputId": "ecaeee80-c7a8-4061-91c6-9e1ff20f575c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 1, 1, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZI0UX7R3f4H",
        "outputId": "6beba741-d363-47d0-fa35-f281f245d557"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data to PyTorch tensors\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.long)"
      ],
      "metadata": {
        "id": "hm8_V0OQ3hby"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`y` are typically converted to a `torch.long` tensor (which is a `64-bit` integer type, equivalent to `torch.int64`) because:\n",
        "\n",
        "- Classification labels are integers (e.g., class indices like 0, 1, 2).\n",
        "\n",
        "- PyTorch loss functions (like `CrossEntropyLoss`) expect labels in integer form, not floats.\n",
        "\n",
        "- Memory efficiency—integers are sufficient for class indices."
      ],
      "metadata": {
        "id": "e-kbClFC2gsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8b5_oEF8ejn",
        "outputId": "3054d342-3b07-43f1-e032-4454eea13e92"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0683, -0.9701],\n",
              "        [-1.1402, -0.8388],\n",
              "        [-2.8954,  1.9769],\n",
              "        [-0.7206, -0.9606],\n",
              "        [-1.9629, -0.9923],\n",
              "        [-0.9382, -0.5430],\n",
              "        [ 1.7273, -1.1858],\n",
              "        [ 1.7774,  1.5116],\n",
              "        [ 1.8997,  0.8344],\n",
              "        [-0.5872, -1.9717]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pkxag9k-8gQh",
        "outputId": "b394cfe7-e585-4293-c41f-76cbbad96b80"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Core Concept**\n",
        "\n",
        "* **Dataset** and **DataLoader** are key abstractions in **PyTorch** that **decouple**:\n",
        "\n",
        "  * ✅ *How you define your data*\n",
        "  * ✅ *How you iterate over it efficiently*\n",
        "\n",
        "---\n",
        "\n",
        "### 📦 **Dataset Class**\n",
        "\n",
        "* Abstract class (acts as a **blueprint**) 🏗️\n",
        "* You define **how data is loaded & returned** 📥📤\n",
        "* Implements:\n",
        "\n",
        "  * `__init__()` – *how data should be loaded* 🧾\n",
        "  * `__len__()` – *total number of samples* 🔢\n",
        "  * `__getitem__(index)` – *fetch data (and label) at index* 🎯\n",
        "* Pulls raw data (e.g., from memory or disk) into rows 🧱➡️📊\n",
        "\n",
        "---\n",
        "\n",
        "### 🚚 **DataLoader Class**\n",
        "\n",
        "* Wraps around `Dataset` to handle:\n",
        "\n",
        "  * Batching 📦\n",
        "  * Shuffling 🔀\n",
        "  * Parallel loading 🧵🧵\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 **DataLoader Control Flow**\n",
        "\n",
        "1. At start of each **epoch**, if `shuffle=True`, it uses a **sampler** 🔀\n",
        "2. Indices split into **chunks** of `batch_size` 📐\n",
        "3. For each index in chunk, data fetched from `Dataset` 🏃‍♂️\n",
        "4. Samples are **collected & combined** into a batch (via `collate_fn`) 🧩\n",
        "5. Final **batch is returned to training loop** 🔁💪\n",
        "\n"
      ],
      "metadata": {
        "id": "GIWn4acGlf72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating Dataset**"
      ],
      "metadata": {
        "id": "cQ3_dVxT4rGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# CustomDataset inherits from PyTorch's Dataset class to handle data loading\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"A custom PyTorch Dataset for loading features and labels.\"\"\"\n",
        "\n",
        "    def __init__(self, features, labels):\n",
        "        \"\"\"\n",
        "        Initialize the dataset with features and labels.\n",
        "\n",
        "        Args:\n",
        "            features (torch.Tensor or np.ndarray): Input data (e.g., images, vectors).\n",
        "            labels (torch.Tensor or np.ndarray): Corresponding labels/targets.\n",
        "        \"\"\"\n",
        "        self.features = features  # Store input features (X)\n",
        "        self.labels = labels      # Store labels/targets (y)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of samples.\n",
        "        \"\"\"\n",
        "        return self.features.shape[0]  # Assumes features is a tensor/array with shape [num_samples, ...]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Fetches a single sample and its label at the given index.\n",
        "\n",
        "        Args:\n",
        "            index (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (features[index], labels[index])\n",
        "            - features[index] (torch.Tensor): Input data for the sample.\n",
        "            - labels[index] (torch.Tensor): Label for the sample.\n",
        "        \"\"\"\n",
        "        return self.features[index], self.labels[index]  # Return (X, y) for the given index"
      ],
      "metadata": {
        "id": "7nm0KeiA3lxj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(X, y)"
      ],
      "metadata": {
        "id": "mbEXolf88_zV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCMLR4fH9Fvo",
        "outputId": "d9dbe87b-5177-472c-f640-b3662702d419"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUpEROQr9NXs",
        "outputId": "49a5a5d5-7de4-4f10-deaa-ba70d56a1671"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-2.8954,  1.9769]), tensor(0))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅**Dataset Class Template**\n",
        "\n",
        "```python\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.features.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.features[index]\n",
        "        y = self.labels[index]\n",
        "        return x, y\n",
        "\n",
        "```\n",
        "**Note:**\n",
        "\n",
        "- **In PyTorch, data transformations (e.g., `normalization`, `resizing`, `augmentation`) are typically applied in the `__getitem__` method of a Dataset class.**\n",
        "\n",
        "```python\n",
        "from torchvision import transforms\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels, transform=None):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.transform = transform  # Optional transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.features[index]\n",
        "        y = self.labels[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            x = self.transform(x)  # Apply transform if available\n",
        "        \n",
        "        return x, y\n",
        "\n",
        "# Usage:\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "])\n",
        "dataset = CustomDataset(X, y, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32)\n",
        "```"
      ],
      "metadata": {
        "id": "snwVl29I9M1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating Dataloader**"
      ],
      "metadata": {
        "id": "n1YJG9414von"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⚙️ **PyTorch `DataLoader` Features — Quick Reference Gui**de\n",
        "\n",
        "---\n",
        "\n",
        "### 📦 `dataset`\n",
        "\n",
        "* **What it is**: The `Dataset` object containing your data (must implement `__len__` and `__getitem__`).\n",
        "* **Why it matters**: Defines **how data is accessed**; can be a built-in or custom dataset.\n",
        "* ✅ **Required**\n",
        "\n",
        "---\n",
        "\n",
        "### 🔢 `batch_size`\n",
        "\n",
        "* **What it is**: Number of samples per batch (e.g., `batch_size=32`).\n",
        "* **Why it matters**: Controls **memory usage** and **training speed**.\n",
        "* 💡 Tip: Larger batch size = faster training, but needs more memory.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔀 `shuffle`\n",
        "\n",
        "* **What it is**: Boolean (`True/False`) to shuffle data at the start of each epoch.\n",
        "* **Why it matters**: Prevents **model overfitting to order**; improves generalization.\n",
        "* ✨ Default: `False`\n",
        "\n",
        "---\n",
        "\n",
        "### 🚀 `num_workers`\n",
        "\n",
        "* **What it is**: Number of subprocesses used to **load data in parallel**.\n",
        "* **Why it matters**: Boosts performance on large datasets.\n",
        "* 🧠 Tip: Try `num_workers=4` or more if your CPU can handle it.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧱 `collate_fn`\n",
        "\n",
        "* **What it is**: A function to **merge** a list of samples into a batch.\n",
        "* **Why it matters**: Needed for **variable-length sequences** (e.g., padding text).\n",
        "* ⚙️ Useful for: NLP, uneven image sizes, custom batching logic.\n",
        "\n",
        "---\n",
        "\n",
        "### 🎲 `sampler`\n",
        "\n",
        "* **What it is**: Object that defines the **strategy for sampling data indices**.\n",
        "* **Why it matters**: Used when you need **custom sampling** (e.g., class balancing).\n",
        "* Example: `RandomSampler`, `SequentialSampler`\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 `batch_sampler`\n",
        "\n",
        "* **What it is**: Like `sampler`, but returns **batches of indices** instead of individual ones.\n",
        "* **Why it matters**: Replaces both `batch_size` and `shuffle` when used.\n",
        "* 🛑 Cannot use `batch_size`, `shuffle`, `sampler` at the same time.\n",
        "\n",
        "---\n",
        "\n",
        "### ❌ `drop_last`\n",
        "\n",
        "* **What it is**: Drops the **last incomplete batch** if dataset size isn’t divisible by `batch_size`.\n",
        "* **Why it matters**: Prevents **uneven batch sizes**, useful for consistent batch-based layers.\n",
        "* Default: `False`\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 `persistent_workers`\n",
        "\n",
        "* **What it is**: Keeps data loading workers alive between epochs.\n",
        "* **Why it matters**: Improves performance by avoiding worker re-spawn overhead.\n",
        "* 🧠 Use with `num_workers > 0`\n",
        "\n",
        "---\n",
        "\n",
        "### ⌛ `timeout`\n",
        "\n",
        "* **What it is**: Max time (in seconds) a worker can take to fetch a batch.\n",
        "* **Why it matters**: Helps catch stuck or slow data pipelines.\n",
        "* Default: `0` (no timeout)\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 `prefetch_factor`\n",
        "\n",
        "* **What it is**: Number of batches to prefetch per worker.\n",
        "* **Why it matters**: Smooths training by keeping the queue full.\n",
        "* Default: `2`\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 `pin_memory`\n",
        "\n",
        "* **What it is**: If `True`, the DataLoader will copy tensors into **CUDA pinned memory** before returning them.\n",
        "* **Why it matters**: Speeds up transfer to GPU.\n",
        "* 🧠 Use when training on GPU!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v5XTesIYEWMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataLoader to efficiently handle data batching and shuffling\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Iterate through the DataLoader to fetch batches of features and labels\n",
        "for batch_features, batch_labels in dataloader:\n",
        "    print(batch_features)  # Print the current batch of features\n",
        "    print(batch_labels)    # Print the corresponding labels\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AE9Ji0l9gVB",
        "outputId": "225590d9-be66-4982-8a02-b560b2bcd85c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.1402, -0.8388],\n",
            "        [-2.8954,  1.9769]])\n",
            "tensor([0, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[ 1.7273, -1.1858],\n",
            "        [ 1.8997,  0.8344]])\n",
            "tensor([1, 1])\n",
            "--------------------------------------------------\n",
            "tensor([[ 1.7774,  1.5116],\n",
            "        [-1.9629, -0.9923]])\n",
            "tensor([1, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[-0.7206, -0.9606],\n",
            "        [-0.9382, -0.5430]])\n",
            "tensor([0, 1])\n",
            "--------------------------------------------------\n",
            "tensor([[ 1.0683, -0.9701],\n",
            "        [-0.5872, -1.9717]])\n",
            "tensor([1, 0])\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Optimizing**  [`03_PyTorch_Traininga_Pipeline`](https://drive.google.com/file/d/1Thw9YVUJPKRrzWPWdwGVQJrTbfnE3xMn/view?usp=sharing) **code**\n"
      ],
      "metadata": {
        "id": "aN_9xjRSFnDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Optimized code for Breast Cancer Classification using PyTorch\n",
        "\"\"\"\n",
        "\n",
        "# === Import Required Libraries ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder"
      ],
      "metadata": {
        "id": "HfPs_OsNf5Sa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Constants ===\n",
        "DATA_URL = \"https://raw.githubusercontent.com/mohd-faizy/PyTorch-Essentials/refs/heads/main/PyTorch_x1/_dataset/Breast-Cancer-Detection.csv\"\n",
        "DATA_FILE = \"Breast_Cancer_Detection.csv\""
      ],
      "metadata": {
        "id": "xOKCDQ5pf6_Z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters & Configurations\n",
        "TEST_SIZE = 0.2        # 20% of data for testing\n",
        "SEED = 42              # Random seed for reproducibility\n",
        "LEARNING_RATE = 0.1    # Learning rate for optimizer\n",
        "EPOCHS = 100           # Number of training epochs\n",
        "BATCH_SIZE = 32        # Batch size for DataLoader"
      ],
      "metadata": {
        "id": "81Uwe2wnf8nK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Download Dataset (if not already present) ===\n",
        "import os\n",
        "if not os.path.exists(DATA_FILE):\n",
        "    os.system(f\"wget -q -O {DATA_FILE} {DATA_URL}\")"
      ],
      "metadata": {
        "id": "qmwojjrlgAoh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Data Preprocessing Function ===\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses the breast cancer dataset.\n",
        "\n",
        "    Steps:\n",
        "    1. Load CSV into a DataFrame.\n",
        "    2. Drop unnecessary columns.\n",
        "    3. Split into features (X) and labels (y).\n",
        "    4. Standardize features (mean=0, std=1).\n",
        "    5. Encode labels (e.g., 'M'/'B' → 1/0).\n",
        "    6. Convert to PyTorch tensors.\n",
        "\n",
        "    Returns:\n",
        "        X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor\n",
        "    \"\"\"\n",
        "    # --- Load Data ---\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Dataset Shape: {df.shape}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    # --- Drop Unnecessary Columns ---\n",
        "    df.drop(columns=['id', 'Unnamed: 32'], inplace=True)\n",
        "\n",
        "    # --- Separate Features (X) and Labels (y) ---\n",
        "    X = df.iloc[:, 1:]  # All columns except 'diagnosis'\n",
        "    y = df.iloc[:, 0]   # Only 'diagnosis' column (labels)\n",
        "\n",
        "    # --- Train-Test Split ---\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, random_state=SEED\n",
        "    )\n",
        "    print(f\"\\nTrain/Test Split:\")\n",
        "    print(f\"  X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
        "    print(f\"  y_train: {y_train.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "    # --- Standardize Features ---\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # --- Encode Labels (M=1, B=0) ---\n",
        "    encoder = LabelEncoder()\n",
        "    y_train = encoder.fit_transform(y_train)\n",
        "    y_test = encoder.transform(y_test)\n",
        "\n",
        "    # --- Convert to PyTorch Tensors ---\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Shape: (n, 1)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    print(\"\\nTensor Shapes:\")\n",
        "    print(f\"  X_train_tensor: {X_train_tensor.shape}\")\n",
        "    print(f\"  X_test_tensor: {X_test_tensor.shape}\")\n",
        "    print(f\"  y_train_tensor: {y_train_tensor.shape}\")\n",
        "    print(f\"  y_test_tensor: {y_test_tensor.shape}\")\n",
        "\n",
        "    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor\n",
        "\n",
        "# --- Execute Data Loading ---\n",
        "# if __name__ == \"__main__\":\n",
        "#     X_train, X_test, y_train, y_test = load_and_preprocess_data(DATA_FILE)"
      ],
      "metadata": {
        "id": "E0cuZiaKUb5k"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Custom Dataset Class ===\n",
        "class CustomDataset(Dataset):      # Inherits from PyTorch's Dataset class\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features   # Stores the input features/Data\n",
        "        self.labels = labels       # Stores the corresponding labels/targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)  # Returns the total number of samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.features[idx]     # Gets the features for the given index\n",
        "        y = self.labels[idx]       # Gets the corresponding label\n",
        "\n",
        "        return x, y                # Returns the (features, label) tuple"
      ],
      "metadata": {
        "id": "Jcw0xo-1gaVz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BreastCancerClassifier(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(num_features, 64),    # First hidden layer\n",
        "            nn.BatchNorm1d(64),             # Batch normalization\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),              # Dropout layer\n",
        "\n",
        "            nn.Linear(64, 32),              # Second hidden layer\n",
        "            nn.BatchNorm1d(32),             # Batch normalization\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),              # Dropout layer\n",
        "\n",
        "            nn.Linear(32, 1),               # Output layer\n",
        "            nn.Sigmoid()                    # Sigmoid for binary classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "CpfBln5UoWyS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load and Prepare Data ===\n",
        "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = load_and_preprocess_data(DATA_FILE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S9FD0hXrJzI",
        "outputId": "ce90f137-cdc8-467e-c358-2a7412335e84"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (569, 33)\n",
            "Columns: ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32']\n",
            "\n",
            "Train/Test Split:\n",
            "  X_train: (455, 30), X_test: (114, 30)\n",
            "  y_train: (455,), y_test: (114,)\n",
            "\n",
            "Tensor Shapes:\n",
            "  X_train_tensor: torch.Size([455, 30])\n",
            "  X_test_tensor: torch.Size([114, 30])\n",
            "  y_train_tensor: torch.Size([455, 1])\n",
            "  y_test_tensor: torch.Size([114, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Dataset ===\n",
        "train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = CustomDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# === Dataloader ===\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "9to8XdwDrx0d"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Model Initialization and Training Setup ===\n",
        "model = BreastCancerClassifier(X_train_tensor.shape[1]) # ~ X_train_tensor.shape[1] = 30 -> no. of Features\n",
        "print(model.parameters)\n",
        "# print(model.model[0])\n",
        "print(model.model[8].weight)\n",
        "print(model.model[8].bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH_lCLDPthTc",
        "outputId": "bca1b371-a31d-492d-b60a-01f3cd0e1f80"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Module.parameters of BreastCancerClassifier(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=30, out_features=64, bias=True)\n",
            "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.3, inplace=False)\n",
            "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): Dropout(p=0.3, inplace=False)\n",
            "    (8): Linear(in_features=32, out_features=1, bias=True)\n",
            "    (9): Sigmoid()\n",
            "  )\n",
            ")>\n",
            "Parameter containing:\n",
            "tensor([[ 0.1505, -0.0251,  0.1622,  0.0267, -0.0456,  0.0881,  0.1491,  0.1192,\n",
            "         -0.0549, -0.0446, -0.0063,  0.0410, -0.1267,  0.0157,  0.0715, -0.0696,\n",
            "          0.0421, -0.1363,  0.0172,  0.0828,  0.0565,  0.1614,  0.0347, -0.1035,\n",
            "         -0.0318, -0.0010,  0.1106, -0.1314, -0.0382,  0.0725,  0.1170,  0.0768]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0747], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # loss function and Optimizer\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "r-rc33VZsoX_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Training Loop ===\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()  # Set model to training mode (enables dropout/batch norm behavior)\n",
        "\n",
        "    # Iterate over batches of training data\n",
        "    for batch_features, batch_labels in train_loader:\n",
        "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "        y_pred = model(batch_features)\n",
        "\n",
        "        # Compute loss by comparing predictions with true labels\n",
        "        loss = loss_function(y_pred, batch_labels)\n",
        "\n",
        "        # Zero the gradients before backward pass (PyTorch accumulates gradients otherwise)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass: compute gradient of loss w.r.t model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model parameters (perform optimization step)\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print training progress every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch + 1} | Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY1rY9IZvPxN",
        "outputId": "b21dfe59-721e-4364-f1ea-27ba419180c5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Loss: 0.0304\n",
            "Epoch 20 | Loss: 0.2279\n",
            "Epoch 30 | Loss: 0.0091\n",
            "Epoch 40 | Loss: 0.0027\n",
            "Epoch 50 | Loss: 0.0047\n",
            "Epoch 60 | Loss: 0.0251\n",
            "Epoch 70 | Loss: 0.1800\n",
            "Epoch 80 | Loss: 0.0293\n",
            "Epoch 90 | Loss: 0.0039\n",
            "Epoch 100 | Loss: 2.3396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Evaluation ===\n",
        "model.eval()\n",
        "accuracy_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_features, batch_labels in test_loader:\n",
        "        y_pred = model(batch_features)\n",
        "        y_pred = (y_pred > 0.5).float() # Use 0.5 as binary threshold\n",
        "\n",
        "        batch_accuracy = (y_pred.view(-1) == batch_labels.view(-1)).float().mean().item()\n",
        "        print(batch_accuracy)\n",
        "        accuracy_list.append(batch_accuracy)\n",
        "\n",
        "print(y_pred[0:5])\n",
        "print(y_pred.shape)\n",
        "print(batch_labels.shape)\n",
        "print(accuracy_list)\n",
        "\n",
        "overall_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "print(f'\\n✅ Final Accuracy on Test Set: {overall_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyIQN9CVwbbi",
        "outputId": "0de5175b-6d5f-49e8-e286-9e98caf1dd8c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "torch.Size([18, 1])\n",
            "torch.Size([18, 1])\n",
            "[0.96875, 0.96875, 0.9375, 1.0]\n",
            "\n",
            "✅ Final Accuracy on Test Set: 0.9688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Quick Summary**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "51TyoFGt6Hxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔍 **Why are PyTorch's `Dataset` and `DataLoader` classes necessary?**\n",
        "\n",
        "In traditional ML training, especially with large datasets, loading the **entire dataset into memory** for batch gradient descent is often:\n",
        "\n",
        "* 🧠 Inefficient\n",
        "* 💾 Impossible due to memory constraints\n",
        "* 🐌 Slower in convergence\n",
        "\n",
        "PyTorch’s `Dataset` and `DataLoader` classes solve these problems by:\n",
        "\n",
        "* 🔄 Loading data in **mini-batches**\n",
        "* 🌀 Managing **transformations**\n",
        "* 🔀 Shuffling data\n",
        "* ⚙️ Supporting **parallel data loading**\n",
        "\n",
        "This effectively **decouples** how you define your data from how you iterate over it during training.\n",
        "\n",
        "---\n",
        "\n",
        "### 🤔 **What is mini-batch gradient descent and how does it differ from batch gradient descent?**\n",
        "\n",
        "| **Method**                         | **Explanation**                                                    |\n",
        "| ---------------------------------- | ------------------------------------------------------------------ |\n",
        "| **Batch Gradient Descent** 🏋️     | Uses **entire dataset** per update → memory-heavy, slow updates    |\n",
        "| **Mini-Batch Gradient Descent** 🧱 | Splits data into **small batches** → faster, more frequent updates |\n",
        "\n",
        "Advantages of Mini-Batch:\n",
        "\n",
        "* ✅ Lower memory usage\n",
        "* 🚀 Faster convergence\n",
        "* 🧬 Better generalization\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 **What is the primary role of the `Dataset` class in PyTorch?**\n",
        "\n",
        "The `Dataset` class is an **abstract blueprint** for loading and accessing data. A custom Dataset:\n",
        "\n",
        "* 🗂️ Knows where the data resides\n",
        "* 🔍 Defines how to access a single data sample via index\n",
        "* 📦 Handles reading from sources like CSVs or image folders\n",
        "\n",
        "---\n",
        "\n",
        "### 🛠️ **What are the key methods required in a custom `Dataset` class?**\n",
        "\n",
        "1. **`__init__`** 📥\n",
        "   Load raw data from its storage location (e.g., CSV, image folders).\n",
        "\n",
        "2. **`__len__`** 📏\n",
        "   Return total number of samples – used by the DataLoader to compute batches.\n",
        "\n",
        "3. **`__getitem__`** 🎯\n",
        "   Retrieve an individual data sample (features + label) by index.\n",
        "\n",
        "---\n",
        "\n",
        "### 📦 **What is the primary role of the `DataLoader` class in PyTorch?**\n",
        "\n",
        "The `DataLoader` acts as a **batch manager**:\n",
        "\n",
        "* 🧮 Decides batch size\n",
        "* 🔀 Shuffles data (if needed)\n",
        "* 🔗 Combines samples into batches\n",
        "* 🧵 Supports parallel loading (`num_workers`)\n",
        "* 📬 Feeds batches to the training loop\n",
        "\n",
        "---\n",
        "\n",
        "### 🤝 **How do `Dataset` and `DataLoader` work together for mini-batch training?**\n",
        "\n",
        "1. The `DataLoader`:\n",
        "\n",
        "   * Optionally **shuffles** sample indices\n",
        "   * Splits them into batches\n",
        "\n",
        "2. It uses the `__getitem__` method from `Dataset` to:\n",
        "\n",
        "   * 🧲 Fetch samples by index\n",
        "   * 🧪 Apply transformations if needed\n",
        "\n",
        "3. The fetched samples are:\n",
        "\n",
        "   * 📦 Collated into a batch tensor\n",
        "   * 🧠 Passed to the training loop\n",
        "\n",
        "---\n",
        "\n",
        "### 🎨 **Where can data transformations be applied in the loading process?**\n",
        "\n",
        "Transformations (e.g., image resizing, augmentation, text cleanup) are typically done in the:\n",
        "\n",
        "* **`__getitem__`** method of your `Dataset` class\n",
        "\n",
        "This ensures:\n",
        "\n",
        "* 🧼 Clean, preprocessed data\n",
        "* 🎯 Applied per-sample, just in time\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "alHow0Oz6vhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FAQs**"
      ],
      "metadata": {
        "id": "HZTaV9_m6wXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📋 Quiz – Questions & Answers\n",
        "\n",
        "---\n",
        "\n",
        "**❓ What is the primary problem with using Batch Gradient Descent in PyTorch for large datasets?**\n",
        "\n",
        "**✅** 🧠 It is memory-inefficient because it loads the **entire dataset into RAM** at once. It also results in **slow convergence** since parameter updates are infrequent.\n",
        "\n",
        "---\n",
        "\n",
        "**❓ What are the two main problems with the manual approach to implementing Mini-Batch Gradient Descent?**\n",
        "\n",
        "**✅** ⚠️ The manual method lacks a **standard interface**, and doesn't handle **transformations, shuffling, batching, or parallelization** efficiently.\n",
        "\n",
        "---\n",
        "\n",
        "**❓ How do the Dataset and DataLoader classes help to solve the problems associated with the manual approach?**\n",
        "\n",
        "**✅** 🔧 `Dataset` provides a **standard way to retrieve data points**, and `DataLoader` manages **batching, shuffling, and parallel data loading**.\n",
        "\n",
        "---\n",
        "\n",
        "**❓ What is the primary role of the Dataset class in the PyTorch data loading process?**\n",
        "\n",
        "**✅** 📂 It **knows where data lives** and defines how to retrieve a **single data point** using an index.\n",
        "\n",
        "---\n",
        "\n",
        "**❓ What is the primary role of the DataLoader class in the PyTorch data loading process?**\n",
        "\n",
        "**✅** 🚚 It **iterates over the Dataset**, handles **batch size**, **shuffling**, and optionally uses **multiple workers** for faster loading.\n",
        "\n",
        "---\n",
        "\n",
        "**❓ When creating a custom Dataset class, what are the three essential methods to implement?**\n",
        "\n",
        "**✅** 🛠️ You must define `__init__`, `__len__`, and `__getitem__`.\n",
        "\n",
        "---\n",
        "\n",
        "**❓ What is the purpose of the `__len__` method in a custom Dataset class?**\n",
        "\n",
        "**✅** 🔢 It returns the **total number of samples** in the dataset, which helps the DataLoader determine the number of batches.\n",
        "\n",
        "---\n",
        "\n",
        "**❓ What is the purpose of the `__getitem__` method in a custom Dataset class?**\n",
        "\n",
        "**✅** 🎯 It retrieves a **specific data point (features and labels)** using the provided index.\n",
        "\n",
        "---\n",
        "\n",
        "**❓ How does the DataLoader handle shuffling of data before creating batches?**\n",
        "\n",
        "**✅** 🔀 It uses a **Sampler** (like `RandomSampler` when `shuffle=True`) to **randomize indices** at the beginning of each epoch.\n",
        "\n",
        "---\n",
        "\n",
        "**❓ What is the function of the `collate_fn` parameter in the DataLoader, and why might you need to customize it?**\n",
        "\n",
        "**✅** 🧩 `collate_fn` combines samples into a batch. You customize it for **irregular data**, like **padding text or variable image sizes**.\n",
        "\n",
        "---\n",
        "\n",
        "### 📝 Essay Questions\n",
        "\n",
        "---\n",
        "\n",
        "**🧠 1. Explain the concept of mini-batch gradient descent and how PyTorch’s Dataset and DataLoader facilitate it.**\n",
        "Mini-batch gradient descent updates model parameters using small subsets of the data, improving convergence speed and memory efficiency. PyTorch’s `Dataset` and `DataLoader` simplify this by providing tools for accessing and batching data seamlessly, unlike full batch gradient descent which is slow and memory-heavy.\n",
        "\n",
        "---\n",
        "\n",
        "**🔄 2. Describe the workflow of data loading and batch creation in PyTorch using a custom Dataset and a DataLoader.**\n",
        "Data is first organized using a custom `Dataset` class. The `DataLoader` then retrieves batches of data, optionally shuffling and parallelizing loading, feeding batches to the model during training.\n",
        "\n",
        "---\n",
        "\n",
        "**⚙️ 3. Discuss the importance of `__init__`, `__len__`, and `__getitem__` in a custom Dataset.**\n",
        "\n",
        "* `__init__` sets up the data source.\n",
        "* `__len__` returns the dataset size.\n",
        "* `__getitem__` returns a specific data sample.\n",
        "  Together, they enable consistent and scalable data access.\n",
        "\n",
        "---\n",
        "\n",
        "**🚀 4. Analyze the benefits of using the DataLoader’s `num_workers` parameter.**\n",
        "Using multiple workers allows **parallel data loading**, reducing I/O bottlenecks and making training faster—especially helpful when dealing with large datasets or expensive preprocessing.\n",
        "\n",
        "---\n",
        "\n",
        "**🧪 5. Illustrate a non-text example where a custom `collate_fn` is necessary.**\n",
        "In image datasets where input sizes vary, a custom `collate_fn` could **resize or crop** images during batching to ensure uniform shape. Without it, batch formation would fail.\n",
        "\n",
        "---\n",
        "\n",
        "### 📚 Glossary of Key Terms\n",
        "\n",
        "* **🔁 Batch Gradient Descent** – Uses the **entire dataset** to compute gradients.\n",
        "* **📦 Mini-Batch Gradient Descent** – Uses **small batches** to update model weights.\n",
        "* **🧰 Dataset Class** – Interface to access individual data points.\n",
        "* **🚚 DataLoader Class** – Handles batching, shuffling, and parallel loading.\n",
        "* **📐 Tensor** – Core data structure in PyTorch for numerical computation.\n",
        "* **📈 Autograd Module** – Computes gradients automatically.\n",
        "* **🛤️ Training Pipeline** – Sequence: load data → model → loss → backprop → update.\n",
        "* **🔄 Epoch** – One full pass through the dataset.\n",
        "* **⏩ Forward Pass** – Model generates predictions.\n",
        "* **❌ Loss Calculation** – Measures prediction error.\n",
        "* **🔙 Backward Pass** – Computes gradients of loss w\\.r.t. parameters.\n",
        "* **🔧 Parameter Update** – Adjusts weights using gradients.\n",
        "* **📊 Batch Size** – Number of samples processed per iteration.\n",
        "* **🪄 Transformation** – Preprocessing (e.g., normalization, resizing).\n",
        "* **🔀 Shuffling** – Randomizes data order before training.\n",
        "* **🎯 Sampling** – Selects subsets of data using a defined strategy.\n",
        "* **🚀 Parallelization** – Speeds up data loading by using multiple threads.\n",
        "* **🛠️ Custom Dataset** – User-defined Dataset for specific formats or sources.\n",
        "* **🧱 Constructor (`__init__`)** – Initializes internal dataset structure.\n",
        "* **🔢 Length Method (`__len__`)** – Returns total number of items in dataset.\n",
        "* **🔍 Get Item (`__getitem__`)** – Returns data sample for a given index.\n",
        "* **🧲 Sampler** – Defines the index selection strategy.\n",
        "* **🎲 Random Sampler** – Random selection without replacement.\n",
        "* **📜 Sequential Sampler** – Ordered sampling of data.\n",
        "* **🧬 Collate Function (`collate_fn`)** – Combines samples into batches.\n",
        "* **🧊 Padding** – Fills short sequences to match batch shape.\n",
        "* **👥 Num Workers** – Subprocesses used to load data in parallel.\n",
        "* **✂️ Drop Last** – Whether to ignore the final incomplete batch.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sRllkGGb7IlY"
      }
    }
  ]
}