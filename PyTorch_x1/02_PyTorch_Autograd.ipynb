{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🔥**Autograd**\n"
      ],
      "metadata": {
        "id": "_Rt59xWdiu10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is Autograd & why its used?**\n",
        "\n",
        "- **Gradient:** The derivative of a function with respect to its inputs\n",
        "\n",
        "- Autograd in PyTorch is its **automatic differentiation engine**, which is essential for training deep learning models. It enables **automatic computation of gradients**, making backpropagation efficient and easy, which is essential for the training machine learning models using optimization algorithms like gradient descent.\n",
        "\n",
        "- Autograd in PyTorch solves the problem of **manually calculating derivatives (gradients)** in neural networks.\n",
        "\n",
        "- Neural networks are essentially **complex, nested functions**, and training them requires finding the derivative of the loss function with respect to the model's parameters (weights and biases).\n",
        "\n",
        "- Calculating these derivatives manually is **difficult and almost impossible** for large networks due to the need for the **chain rule**.\n",
        "\n",
        "- Autograd provides **automatic differentiation**, allowing for the automatic calculation of these essential gradients for optimization algorithms like Gradient Descent used in training.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6NWbn5i3xk1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How Autograd Works in PyTorch**\n",
        "\n",
        "#### **1. Tracking Operations**  \n",
        "- Autograd **records all operations** performed on tensors with `requires_grad=True`.  \n",
        "- These operations form a **computational graph**, where:  \n",
        "  - **Nodes** are tensors (inputs, intermediates, outputs).  \n",
        "  - **Edges** represent the operations (functions) transforming them.  \n",
        "\n",
        "#### **2. Building a Dynamic Computation Graph**  \n",
        "- PyTorch constructs a **Directed Acyclic Graph (DAG)** in real-time as operations are executed.  \n",
        "- Each tensor stores:  \n",
        "  - Its data (`grad_fn` for the operation that created it).  \n",
        "  - A reference to the backward function (`grad_fn`) for gradient computation.  \n",
        "\n",
        "#### **3. Computing Gradients with `.backward()`**  \n",
        "- When `.backward()` is called, Autograd:  \n",
        "  1. **Traverses the graph in reverse** (from output to input).  \n",
        "  2. **Applies the chain rule** to compute gradients for all tracked tensors.  \n",
        "  3. **Stores gradients** in the `.grad` attribute of leaf tensors (parameters).  \n",
        "\n",
        "#### **4. Updating Model Parameters**  \n",
        "- Gradients are used by optimizers (e.g., **SGD, Adam**) to update weights via:  \n",
        "  ```python\n",
        "  optimizer.step()  # Updates parameters using gradients\n",
        "  optimizer.zero_grad()  # Clears old gradients\n",
        "  ```\n",
        "\n",
        "### **Key Features**  \n",
        "- **Dynamic Graph**: The graph is rebuilt on-the-fly in each forward pass (*define-by-run*).  \n",
        "- **Efficiency**: Only computes gradients for tensors with `requires_grad=True`.  \n",
        "- **Memory Management**: Intermediate values are discarded after backward pass unless retained (e.g., for double backward)."
      ],
      "metadata": {
        "id": "pjbv101EAaIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Why PyTorch make the computation graphs?**\n",
        "\n",
        "PyTorch creates computation graphs (specifically, directed acyclic graphs or DAGs) to track the operations performed on tensors. This tracking allows the Autograd module to automatically calculate derivatives (gradients) by tracing the graph backward from the output (roots) to the input (leaves) using the chain rule. This automatic differentiation is essential for efficiently training neural networks as manually calculating gradients for complex, nested network functions is difficult."
      ],
      "metadata": {
        "id": "vSRW4cxhVDXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅**Example: 1**\n",
        "\n"
      ],
      "metadata": {
        "id": "3ZxQ2iIR0MoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write a program to calculate the gradient `dy/dx`?**\n",
        "\n",
        "- $y = x^2$\n",
        "\n",
        "    - $\\frac{dy}{dx} = 2x$\n"
      ],
      "metadata": {
        "id": "uaRQOjEtAJLO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UXPFYGopf4KV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b2d1fc-8a86-48f5-b2ac-d7a4cf2220d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "def dy_dx(x):\n",
        "  return 2*x\n",
        "\n",
        "dy_dx(2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Autograd\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = x**2\n",
        "\n",
        "print(\"x -> \", x)\n",
        "print(\"y -> \", y)\n",
        "\n",
        "y.backward()\n",
        "print(\"x.grad -> \", x.grad)"
      ],
      "metadata": {
        "id": "bm_ra2CG21GV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71ad40f-5388-4309-e26c-9cea29a47f94"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x ->  tensor(3., requires_grad=True)\n",
            "y ->  tensor(9., grad_fn=<PowBackward0>)\n",
            "x.grad ->  tensor(6.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅**Example: 2**\n",
        "\n"
      ],
      "metadata": {
        "id": "82HMkjJN4lfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write a program to calculate the gradient `dz/dx`?**\n",
        "\n",
        "- $y = x^2$\n",
        "\n",
        "    - $\\frac{dy}{dx} = 2x$\n",
        "\n",
        "- $z = Sin(y)$\n",
        "    - $\\frac{dz}{dy} = Cos(y)$\n",
        "\n",
        "- $\\frac{dz}{dx} = \\frac{dz}{dy} . \\frac{dy}{dx} = 2x.Cos(y) = 2x.Cos(x^2)$"
      ],
      "metadata": {
        "id": "vmePCx-pANXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manual Gradient**"
      ],
      "metadata": {
        "id": "kAR7OBAVpgcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def dz_dx(x):\n",
        "    return 2 * x * math.cos(x**2)\n",
        "\n",
        "dz_dx(4)"
      ],
      "metadata": {
        "id": "d_66YDf8gFwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a05c20ad-47ad-4dcb-ea98-1d7fc7d08138"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-7.661275842587077"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Autograd**"
      ],
      "metadata": {
        "id": "aIar9-D0pjwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Autograd demonstration\n",
        "\n",
        "import torch\n",
        "\n",
        "# Input tensor with gradient tracking\n",
        "x = torch.tensor(4.0, requires_grad=True)\n",
        "# Operation 1\n",
        "y = x ** 2\n",
        "# Operation 2\n",
        "z = torch.sin(y)\n",
        "\n",
        "print(\"x:\", x)\n",
        "print(\"y:\", y)\n",
        "print(\"z:\", z)\n",
        "\n",
        "# Compute gradients\n",
        "z.backward()\n",
        "\n",
        "# Gradient of z w.r.t. x\n",
        "print(\"x.grad:\", x.grad)"
      ],
      "metadata": {
        "id": "tYHUyJhBgO9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e68ce155-7c1d-4726-ec33-25097c6701b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor(4., requires_grad=True)\n",
            "y: tensor(16., grad_fn=<PowBackward0>)\n",
            "z: tensor(-0.2879, grad_fn=<SinBackward0>)\n",
            "x.grad: tensor(-7.6613)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> `PowBackward0` and `SinBackward0` refer to the backward computation steps that PyTorch uses to track gradients for the power and sine functions"
      ],
      "metadata": {
        "id": "mLFy1FIJRU05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGO166vr-pvc",
        "outputId": "9dfef7cb-d25d-4c9b-8a09-34e9b67be7b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-10b3a7061f6d>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  y.grad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nodes PyTorch:**\n",
        "\n",
        "- 🟡**`Leaf nodes`**: These are the **input tensors** to the computation. By default, gradients are calculated and stored for these during the backward pass.\n",
        "- 🟡**`Root nodes`**: This is the **output tensor** (or tensors) from which the backward calculation starts.\n",
        "- 🟡**`Intermediate nodes`**: These represent the **intermediate tensors** in the computation graph, between the input and output.\n",
        "    - *Gradients are typically **not calculated or stored** for these intermediate nodes by default.*\n",
        "\n",
        "\n",
        ">  PyTorch's Autograd builds a directed acyclic graph (DAG) where **leaf nodes are the input tensors** and **root nodes are the output tensors**. In this example, `x` is the input (leaf), `z` is the final output (root), and `y` is an intermediate step.\n",
        "\n",
        "By default, Autograd **does not calculate or store gradients for intermediate tensors**. The gradient calculation happens from the root back to the leaf tensors. Therefore, you can access `x.grad` (a leaf tensor) and `z.grad` (if it were a leaf, though here it's the root from which the backward pass starts), but not `y.grad` because it's an intermediate node."
      ],
      "metadata": {
        "id": "MbfQU1_dVtjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅**Example: 3**"
      ],
      "metadata": {
        "id": "WRrOyEPCAfsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🟡 **Training Process for a Neural Network**\n",
        "\n",
        "1. **Forward Pass** – Compute the output of the network for a given input.  \n",
        "2. **Loss Calculation** – Measure the error using a loss function.  \n",
        "3. **Backward Pass** – Compute the gradients of the loss with respect to the model parameters.  \n",
        "4. **Parameter Update** – Adjust the parameters using an optimizer (e.g., gradient descent).\n",
        "\n",
        "---\n",
        "\n",
        "### ⭕ **Gradient Computation for Weights and Bias**\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔹 **Forward Pass Computations**\n",
        "\n",
        "1. **Linear Transformation**  \n",
        "   $$\n",
        "   z = w \\cdot x + b\n",
        "   $$  \n",
        "   Gradients:  \n",
        "   $$\n",
        "   \\frac{dz}{dw} = x, \\quad \\frac{dz}{db} = 1\n",
        "   $$\n",
        "\n",
        "2. **Activation (Sigmoid Function)**  \n",
        "   $$\n",
        "   \\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$  \n",
        "   Gradient:  \n",
        "   $$\n",
        "   \\frac{d\\hat{y}}{dz} = \\hat{y}(1 - \\hat{y})\n",
        "   $$\n",
        "\n",
        "3. **Loss Function (Binary Cross-Entropy)**  \n",
        "   $$\n",
        "   L = -\\left[y \\cdot \\ln(\\hat{y}) + (1 - y) \\cdot \\ln(1 - \\hat{y})\\right]\n",
        "   $$  \n",
        "   Gradient:  \n",
        "   $$\n",
        "   \\frac{dL}{d\\hat{y}} = \\frac{\\hat{y} - y}{\\hat{y}(1 - \\hat{y})}\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔹 **Using the Chain Rule**\n",
        "\n",
        "To compute the gradient of the loss with respect to the weights and bias:\n",
        "\n",
        "- **Gradient with respect to weight $w$:**\n",
        "  $$\n",
        "  \\frac{dL}{dw} = \\frac{dL}{d\\hat{y}} \\cdot \\frac{d\\hat{y}}{dz} \\cdot \\frac{dz}{dw}\n",
        "  $$\n",
        "\n",
        "- **Gradient with respect to bias $b$:**\n",
        "  $$\n",
        "  \\frac{dL}{db} = \\frac{dL}{d\\hat{y}} \\cdot \\frac{d\\hat{y}}{dz} \\cdot \\frac{dz}{db}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### ⭐ **Final Simplified Gradient Expressions**\n",
        "\n",
        "Since:  \n",
        "$$\n",
        "\\frac{dL}{d\\hat{y}} \\cdot \\frac{d\\hat{y}}{dz} = \\hat{y} - y\n",
        "$$\n",
        "\n",
        "We get:\n",
        "\n",
        "- **Weight Gradient:**\n",
        "  $$\n",
        "  \\frac{dL}{dw} = (\\hat{y} - y) \\cdot x\n",
        "  $$\n",
        "\n",
        "- **Bias Gradient:**\n",
        "  $$\n",
        "  \\frac{dL}{db} = \\hat{y} - y\n",
        "  $$\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "AH0vhjKxfR3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔗**Computational graph**\n",
        "\n",
        "$$\n",
        "\\begin{array}{ccccccccccccccc}\n",
        "\\textbf{w} & \\searrow & & & & & & & \\\\\n",
        " & & \\boxed{\\ast} & \\rightarrow & \\boxed{+} & \\rightarrow & z & \\rightarrow & \\boxed{\\sigma} & \\rightarrow & \\hat{y} & \\rightarrow & \\boxed{LF} & \\rightarrow & \\text{Loss} \\\\\n",
        "\\textbf{x} & \\nearrow & & & \\uparrow & & & & & & & & \\uparrow \\\\\n",
        " & & & & \\textbf{b} & & & & & & & & \\textbf{y} \\\\\n",
        "\\end{array}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "s03UoYVZ4mbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🟩**Manual Gradient**"
      ],
      "metadata": {
        "id": "aiGJJ5J2o0Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Inputs\n",
        "x = torch.tensor(6.7)  # Input feature\n",
        "y = torch.tensor(0.0)  # True label (binary)\n",
        "\n",
        "w = torch.tensor(1.0)  # Weight\n",
        "b = torch.tensor(0.0)  # Bias\n",
        "\n",
        "# Binary Cross-Entropy Loss for scalar\n",
        "def binary_cross_entropy_loss(prediction, target):\n",
        "    epsilon = 1e-8  # To prevent log(0)\n",
        "    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n",
        "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))\n",
        "\n",
        "\n",
        "# Forward pass\n",
        "z = w * x + b  # Weighted sum (linear part)\n",
        "y_pred = torch.sigmoid(z)  # Predicted probability\n",
        "\n",
        "# Compute binary cross-entropy loss\n",
        "loss = binary_cross_entropy_loss(y_pred, y)\n",
        "print(loss)\n",
        "\n",
        "\n",
        "# Derivatives:\n",
        "# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)\n",
        "dloss_dy_pred = (y_pred - y)/(y_pred*(1-y_pred))\n",
        "\n",
        "# 2. dy_pred/dz: Prediction (y_pred) with respect to z (sigmoid derivative)\n",
        "dy_pred_dz = y_pred * (1 - y_pred)\n",
        "\n",
        "# 3. dz/dw and dz/db: z with respect to w and b\n",
        "dz_dw = x  # dz/dw = x\n",
        "dz_db = 1  # dz/db = 1 (bias contributes directly to z)\n",
        "\n",
        "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
        "dL_db = dloss_dy_pred * dy_pred_dz * dz_db\n",
        "\n",
        "print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\n",
        "print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")"
      ],
      "metadata": {
        "id": "IDA7vJD6TERe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6add0f9-a483-4850-f8dc-38b00f2fc45d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.7012)\n",
            "Manual Gradient of loss w.r.t weight (dw): 6.691762447357178\n",
            "Manual Gradient of loss w.r.t bias (db): 0.998770534992218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🟨**Using AutoGrad**"
      ],
      "metadata": {
        "id": "_bpzZAzGmhLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Autograd for binary cross-entropy loss\n",
        "import torch\n",
        "\n",
        "# Input and target\n",
        "x = torch.tensor(6.7)\n",
        "y = torch.tensor(0.0)\n",
        "\n",
        "# Weights and bais with gradient tracking\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "print(\"Initial weight:\", w)\n",
        "print(\"Initial bias:\", b)\n",
        "\n",
        "# Linear layer output\n",
        "z = w * x + b\n",
        "print(\"Linear output (z):\", z)\n",
        "\n",
        "# Predicted probability\n",
        "y_pred = torch.sigmoid(z)\n",
        "print(\"Predicted probability:\", y_pred)\n",
        "\n",
        "# Binary cross-entropy loss\n",
        "def binary_cross_entropy_loss(prediction, target):\n",
        "    epsilon = 1e-8\n",
        "    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n",
        "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))\n",
        "\n",
        "loss = binary_cross_entropy_loss(y_pred, y)\n",
        "print(\"Loss:\", loss)\n",
        "\n",
        "# Compute gradients\n",
        "loss.backward()\n",
        "\n",
        "# Gradients of the loss w.r.t. weights\n",
        "print(\"Gradient of w:\", w.grad)\n",
        "# Gradient of the loss w.r.t. bias\n",
        "print(\"Gradient of b:\", b.grad)"
      ],
      "metadata": {
        "id": "3xdGCYz8V-Rh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c18ac62-9ed4-4734-c63e-a90ebb8ecd95"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weight: tensor(1., requires_grad=True)\n",
            "Initial bias: tensor(0., requires_grad=True)\n",
            "Linear output (z): tensor(6.7000, grad_fn=<AddBackward0>)\n",
            "Predicted probability: tensor(0.9988, grad_fn=<SigmoidBackward0>)\n",
            "Loss: tensor(6.7012, grad_fn=<NegBackward0>)\n",
            "Gradient of w: tensor(6.6918)\n",
            "Gradient of b: tensor(0.9988)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🟧**Now using vector inputs insted of scalar**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7y9do80d7oDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "y = (x**2).mean()\n",
        "print(y)\n",
        "\n",
        "y.backward()\n",
        "print(x.grad) # we get 3 grad one for each input"
      ],
      "metadata": {
        "id": "yuKGIgDThWq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2432e0-f9d6-45b3-816e-c40ad41c245d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3.], requires_grad=True)\n",
            "tensor(4.6667, grad_fn=<MeanBackward0>)\n",
            "tensor([0.6667, 1.3333, 2.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🟦**Clearing gradient**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xY5OtRf78SCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Clearing gradients means **setting the accumulated gradient values of tensors back to zero**.\n",
        "\n",
        "> We need to do this because PyTorch **accumulates gradients** by adding them up every time `backward()` is called. If we don't clear them before each training step (forward and backward pass), the gradients from previous steps will be included, leading to **incorrect gradient values** for the current step's parameter updates. This ensures that the optimization (like Gradient Descent) uses the correct gradients for the current batch or iteration."
      ],
      "metadata": {
        "id": "h14ebzPy82re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcUAtKSFivZ1",
        "outputId": "f8dd74d2-eebd-408f-93e5-63454ce01cab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cell-1\n",
        "# forward\n",
        "y = x ** 2\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpBNKaVf9aM9",
        "outputId": "82aa7d08-6e97-49c6-c2b5-aa627e194dd4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4., grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cell-2\n",
        "# backward\n",
        "y.backward()"
      ],
      "metadata": {
        "id": "6wIMoiWu9JX9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell-3\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjS939-M9Mx8",
        "outputId": "67725eb1-6da4-4032-a192-c5c5ecfcbc55"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">✅ Since PyTorch accumulates gradients by adding them up every time `y.backward()` is called, running `cell-1`, `cell-2`, and `cell-3` consecutively will result in x.grad changing as follows: `tensor(4.)` → `tensor(8.)` → `tensor(12.)` → `tensor(16.)`"
      ],
      "metadata": {
        "id": "Q769Bvro9-4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cell-4\n",
        "x.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxQMMaFqoB4u",
        "outputId": "2314849d-f680-47ec-94f7-6416c633cb6c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> `x.grad.zero_()` is used to reset the accumulated gradients in PyTorch to zero before each new backward pass. Since PyTorch accumulates gradients by default, not clearing them leads to incorrect updates in optimization. This ensures that only the current batch’s gradients are used for updating the model’s parameters."
      ],
      "metadata": {
        "id": "AVI-zhgX_YPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅**Example 4: Disabling Gradient Tracking**"
      ],
      "metadata": {
        "id": "IColJ3qe_gns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disabling gradient tracking** means **stopping PyTorch's Autograd from recording operations and building the computation graph** for specific tensors or sections of code.\n",
        "\n",
        "> ✅ We need this **primarily during inference or prediction** after a neural network has been trained.\n",
        "\n",
        "> During prediction, we only need to perform a forward pass, and we **don't need to calculate gradients** to update model parameters.\n",
        "\n",
        "> Disabling tracking saves **memory and computational resources** that would otherwise be used for gradient calculation."
      ],
      "metadata": {
        "id": "W1bE0qK6_qKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGyP-K6ooelo",
        "outputId": "651fa851-4ff4-4f14-c110-5b02223cfc0b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL3tQ2LAq0n-",
        "outputId": "bab2e5c9-be11-4a26-f238-2bcbeebd100b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "yyOCApZPr7zm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prQYxP_xr_1l",
        "outputId": "e8d5b3c7-fb4d-4043-c95e-ed701a073c0a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Option 1 - `requires_grad_(False)`**\n",
        "\n"
      ],
      "metadata": {
        "id": "-XxipZEqAUoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.requires_grad_(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-3J1W7BsLiK",
        "outputId": "77afea3b-62e9-4e62-96df-3d2f67c2983c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eqdqlwzsPR_",
        "outputId": "8ed9b1e6-999d-4a4a-9066-6fc48b3fdebf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2"
      ],
      "metadata": {
        "id": "I6hTHHQZsQbP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWh8lxdEsdRa",
        "outputId": "0142a006-c982-4167-eca9-d910d19ef5fe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "DrW7KpiBsd6Z",
        "outputId": "168e3ae8-6446-4c90-bb65-b01524f74d96"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-ab75bb780f4c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Option 2 - `detach()`**"
      ],
      "metadata": {
        "id": "4YMN7wAqAbus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "id": "I_Ct0xomsgw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49cd0556-0300-44e5-a7fa-d6f35f26a66c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = x.detach()\n",
        "z"
      ],
      "metadata": {
        "id": "1pslVLzLsmO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d4de2f7-911c-4c5f-8045-e29c41216025"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2\n",
        "y"
      ],
      "metadata": {
        "id": "tXkAjBEcsp_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6efbc91-1dde-4592-d581-f80dd50ad62b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y1 = z ** 2\n",
        "y1"
      ],
      "metadata": {
        "id": "-zK-PB97su18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8481c5ca-749d-4f1b-878b-84cf948ca096"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "_lQLYSmesxeX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1.backward()"
      ],
      "metadata": {
        "id": "yvhtrhXMszVu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "a2084251-88d7-4698-a228-a91be991970f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-78ed6cb36aaa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Option 3 - `torch.no_grad()`**\n",
        "\n"
      ],
      "metadata": {
        "id": "ExT5qi1gAiE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "id": "PrpCL8cbs0us",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5929fff1-8cbc-4ef8-d47d-1efd60560a1d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    y = x ** 2"
      ],
      "metadata": {
        "id": "n88-a1bxs569"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "UMsS-KPGs-x9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6115807a-3ade-466f-fb8a-bc6351091d0b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()\n",
        "\n",
        "# x.grad"
      ],
      "metadata": {
        "id": "GYN6bHVVs_OA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "68185583-e712-4991-d58e-02e9217810ef"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-605c835615c9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# x.grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅**Example 5: Linear Regression**"
      ],
      "metadata": {
        "id": "vguSfnzSkr3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch  # Import PyTorch for tensor operations and automatic differentiation\n",
        "\n",
        "# Linear regression with autograd\n",
        "# Goal: Learn parameters w and b for the model y = w * x + b\n",
        "\n",
        "# Input data\n",
        "x = torch.tensor([1., 2., 3., 4.])  # Input features (independent variable)\n",
        "y_true = torch.tensor([2., 4., 6., 8.])  # Target values (dependent variable, where y = 2 * x ideally)\n",
        "\n",
        "# Parameters (weights and bias)\n",
        "w = torch.tensor(0.0, requires_grad=True)  # Initialize weight w with gradient tracking enabled\n",
        "b = torch.tensor(0.0, requires_grad=True)  # Initialize bias b with gradient tracking enabled\n",
        "\n",
        "# Forward pass\n",
        "def forward(x):\n",
        "    \"\"\"\n",
        "    Compute model predictions: y_pred = w * x + b\n",
        "    Args:\n",
        "        x (torch.Tensor): Input tensor\n",
        "    Returns:\n",
        "        torch.Tensor: Predicted output\n",
        "    \"\"\"\n",
        "    return w * x + b\n",
        "\n",
        "# Loss function (Mean Squared Error)\n",
        "def loss(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Compute Mean Squared Error (MSE) loss between predictions and true values\n",
        "    Args:\n",
        "        y_pred (torch.Tensor): Predicted values\n",
        "        y_true (torch.Tensor): True target values\n",
        "    Returns:\n",
        "        torch.Tensor: MSE loss\n",
        "    \"\"\"\n",
        "    return ((y_pred - y_true) ** 2).mean()\n",
        "\n",
        "# Training loop\n",
        "learning_rate = 0.01  # Step size for parameter updates\n",
        "epochs = 100  # Number of training iterations\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass: Compute predictions\n",
        "    y_pred = forward(x)  # Predict y using current w and b\n",
        "\n",
        "    # Compute loss: Measure error between predictions and true values\n",
        "    l = loss(y_pred, y_true)  # Calculate MSE loss\n",
        "\n",
        "    # Backward pass: Compute gradients of loss with respect to w and b\n",
        "    l.backward()  # Autograd computes gradients and stores them in w.grad and b.grad\n",
        "\n",
        "    # Update parameters and zero gradients\n",
        "    with torch.no_grad():  # Disable gradient tracking for parameter updates to save memory\n",
        "        w -= learning_rate * w.grad  # Update weight using gradient descent: w = w - learning_rate * gradient\n",
        "        b -= learning_rate * b.grad  # Update bias using gradient descent: b = b - learning_rate * gradient\n",
        "\n",
        "        # Zero gradients to prevent accumulation for the next iteration\n",
        "        w.grad.zero_()  # Clear the gradient of w\n",
        "        b.grad.zero_()  # Clear the gradient of b\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}: w = {w.item():.3f}, loss = {l.item():.3f}')\n",
        "\n",
        "# Print final learned model\n",
        "print(f'Final prediction: y = {w.item():.3f} * x + {b.item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULd-HIp_kzCP",
        "outputId": "924d3686-a9b2-4ac6-c56b-50101bcc70b6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: w = 0.300, loss = 30.000\n",
            "Epoch 10: w = 1.559, loss = 0.833\n",
            "Epoch 20: w = 1.767, loss = 0.075\n",
            "Epoch 30: w = 1.805, loss = 0.052\n",
            "Epoch 40: w = 1.816, loss = 0.049\n",
            "Epoch 50: w = 1.822, loss = 0.046\n",
            "Epoch 60: w = 1.827, loss = 0.043\n",
            "Epoch 70: w = 1.832, loss = 0.041\n",
            "Epoch 80: w = 1.837, loss = 0.038\n",
            "Epoch 90: w = 1.842, loss = 0.036\n",
            "Final prediction: y = 1.846 * x + 0.452\n"
          ]
        }
      ]
    }
  ]
}