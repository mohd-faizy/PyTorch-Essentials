{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  ⭐**`torch.nn`** **module in PyTorch**"
      ],
      "metadata": {
        "id": "ylatRN3cGhbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **`torch.nn`** module is PyTorch’s **neural network toolkit**. It provides all the tools needed to build, train, and manage neural networks efficiently.\n",
        "\n",
        "\n",
        "## `torch.nn` Module\n",
        "\n",
        "✅ Simplifies the creation of deep learning models\n",
        "✅ Provides pre-built **layers**, **activation functions**, **losses**, **utilities**\n",
        "✅ Clean, modular, and easy to debug\n",
        "\n",
        "\n",
        "## 🔑 Key Components of `torch.nn`\n",
        "\n",
        "\n",
        "### 1. 🧱 `nn.Module` – The Core Building Block\n",
        "\n",
        "* Base class for **all custom layers and models**\n",
        "* You subclass `nn.Module` to define your own model\n",
        "* Must override:\n",
        "\n",
        "  * `__init__(self)` – define layers\n",
        "  * `forward(self, x)` – define forward pass\n",
        "\n",
        "📌 Think of it like a recipe for how data flows through your network.\n",
        "\n",
        "```python\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(10, 2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "```\n",
        "\n",
        "\n",
        "### 2. 🧩 Common Layers (`torch.nn`)\n",
        "\n",
        "* `nn.Linear(in, out)` ➡️ Fully connected layer\n",
        "* `nn.Conv2d(in_channels, out_channels, kernel_size)` ➡️ Convolution layer\n",
        "* `nn.LSTM(input_size, hidden_size)` ➡️ Recurrent layer\n",
        "* `nn.Embedding(num_embeddings, embedding_dim)` ➡️ Used in NLP\n",
        "* `nn.BatchNorm2d(num_features)` ➡️ Normalizes activations\n",
        "\n",
        "\n",
        "### 3. ⚡ Activation Functions\n",
        "\n",
        "Used to introduce **non-linearity** (helps model learn complex patterns):\n",
        "\n",
        "* `nn.ReLU()`\n",
        "* `nn.Sigmoid()`\n",
        "* `nn.Tanh()`\n",
        "* `nn.LeakyReLU()`\n",
        "* `nn.Softmax(dim=1)`\n",
        "\n",
        "💡 Most activations are also available in `torch.nn.functional` (see below).\n",
        "\n",
        "\n",
        "### 4. 🎯 Loss Functions\n",
        "\n",
        "Loss functions measure the error of predictions:\n",
        "\n",
        "* `nn.CrossEntropyLoss()` ➡️ Classification (auto applies `LogSoftmax + NLLLoss`)\n",
        "* `nn.MSELoss()` ➡️ Regression\n",
        "* `nn.NLLLoss()` ➡️ Negative Log Likelihood\n",
        "* `nn.BCELoss()` ➡️ Binary classification\n",
        "* `nn.HingeEmbeddingLoss()`, `nn.SmoothL1Loss()` ➡️ Advanced cases\n",
        "\n",
        "\n",
        "### 5. 📦 Container Modules\n",
        "\n",
        "Used to **wrap multiple layers** together:\n",
        "\n",
        "* `nn.Sequential(layers...)` ➡️ Runs layers in order\n",
        "* `nn.ModuleList([layer1, layer2])` ➡️ Stores layers in a list\n",
        "* `nn.ModuleDict({\"layer1\": layer1, \"layer2\": layer2})` ➡️ Stores layers in a dict\n",
        "\n",
        "\n",
        "### 6. 🎮 `torch.nn.functional` (a.k.a. `F`)\n",
        "\n",
        "* Contains **functions** (not classes) for activations, loss, etc.\n",
        "* Use when you need **more control** or **lightweight operations** without storing parameters\n",
        "\n",
        "✅ Examples:\n",
        "\n",
        "```python\n",
        "import torch.nn.functional as F\n",
        "\n",
        "F.relu(x)\n",
        "F.cross_entropy(output, target)\n",
        "F.softmax(x, dim=1)\n",
        "```\n",
        "\n",
        "💡 Use `F.*` inside `forward()` if you're not declaring layers in `__init__`.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. 🛡️ Regularization: Dropout & Normalization\n",
        "\n",
        "* `nn.Dropout(p=0.5)` ➡️ Randomly zeroes some neurons to reduce overfitting\n",
        "* `nn.BatchNorm1d/2d/3d` ➡️ Stabilizes training, faster convergence\n",
        "* `nn.LayerNorm()` ➡️ Used often in NLP models like transformers\n",
        "\n",
        "\n",
        "### 8. 🔄 Weight Initialization (Optional but important)\n",
        "\n",
        "You can customize weights manually using:\n",
        "\n",
        "```python\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "```\n",
        "\n",
        "Then apply with:\n",
        "\n",
        "```python\n",
        "model.apply(init_weights)\n",
        "```\n",
        "\n",
        "\n",
        "### 9. 🧪 Model Training Pattern (Quick Template)\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. Define Model\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# 2. Initialize\n",
        "model = MyNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 3. Training Loop (simplified)\n",
        "for data, labels in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(data)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "```\n",
        "\n",
        "\n",
        "### ✅ Summary Cheat Sheet\n",
        "\n",
        "| Component             | Purpose                              |\n",
        "| --------------------- | ------------------------------------ |\n",
        "| `nn.Module`           | Base class for models                |\n",
        "| `nn.Linear`, etc.     | Layers to build the model            |\n",
        "| `nn.ReLU`, etc.       | Activation functions                 |\n",
        "| `nn.CrossEntropy`     | Loss function for classification     |\n",
        "| `nn.Sequential`       | Layer stack container                |\n",
        "| `F.relu`, `F.softmax` | Functional API (no params stored)    |\n",
        "| `nn.Dropout`          | Regularization to reduce overfitting |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ft-jmoxZGUVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Summary\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcER5vK3saCz",
        "outputId": "d7f9cb47-4bac-491d-8b24-8580f7c0a935"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model 1**"
      ],
      "metadata": {
        "id": "OUwG8SLRh56Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries from PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a custom model class inheriting from nn.Module\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()  # Essential to initialize the base nn.Module class so that all its internal machinery (like parameter registration) works correctly\n",
        "\n",
        "        # Define a fully connected layer (Linear layer) that maps input features to a single output\n",
        "        self.layer = nn.Linear(num_features, 1) # input -> 5, ouput -> 1\n",
        "\n",
        "        # Apply sigmoid activation to squash output between 0 and 1 (useful for binary classification)\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    # Define the forward pass that determines how input data flows through the model\n",
        "    def forward(self, features):\n",
        "        out = self.layer(features)         # Apply linear transformation\n",
        "        out = self.activation(out)         # Apply sigmoid non-linearity\n",
        "        return out"
      ],
      "metadata": {
        "id": "innQxNbGfDdf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy dataset (10 samples, each with 5 features)\n",
        "features = torch.rand(10, 5)\n",
        "print(features)\n",
        "print(features.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDNUCBZhFoGs",
        "outputId": "5faa8e8f-321e-4938-b724-044d34a519e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2593, 0.6335, 0.0525, 0.0240, 0.7354],\n",
            "        [0.5317, 0.6713, 0.6719, 0.4471, 0.4876],\n",
            "        [0.4134, 0.0661, 0.7174, 0.1425, 0.7884],\n",
            "        [0.6656, 0.1367, 0.9670, 0.0442, 0.3053],\n",
            "        [0.0266, 0.2185, 0.6821, 0.5872, 0.4163],\n",
            "        [0.5958, 0.3406, 0.5305, 0.7849, 0.3152],\n",
            "        [0.0372, 0.1772, 0.1487, 0.5070, 0.3187],\n",
            "        [0.5605, 0.9877, 0.9243, 0.2497, 0.7496],\n",
            "        [0.1990, 0.0298, 0.6669, 0.4172, 0.3709],\n",
            "        [0.1810, 0.5004, 0.7821, 0.6057, 0.7022]])\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Model\n",
        "# model1 = Model(5) # better approch is to use `features.shape[1]`\n",
        "model1 = Model(features.shape[1])"
      ],
      "metadata": {
        "id": "Hheop11SFgde"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call model for forward pass\n",
        "# we send 10 rows ~ we get 10 corresponding output\n",
        "print(model1(features))        # Output of the model after sigmoid activation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWaGwYgQFiYU",
        "outputId": "d33580a5-d139-41f5-eb47-bbafed5591f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3802],\n",
            "        [0.3929],\n",
            "        [0.3981],\n",
            "        [0.4450],\n",
            "        [0.4323],\n",
            "        [0.3800],\n",
            "        [0.4162],\n",
            "        [0.3890],\n",
            "        [0.4348],\n",
            "        [0.3961]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model1.layer.weight)     # Print the learned weights of the linear layer\n",
        "print(model1.layer.bias)       # Print the learned bias of the linear layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiFPZTA2IWun",
        "outputId": "e273de56-a76c-4ca8-8890-8cb98bd0d1d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.2989, -0.0333,  0.2284, -0.2418, -0.3920]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1079], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model-1](https://raw.githubusercontent.com/mohd-faizy/PyTorch-Essentials/refs/heads/main/PyTorch_x1/_img/04_1.jpeg)"
      ],
      "metadata": {
        "id": "c8m33k9wnJjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 1 Summary**"
      ],
      "metadata": {
        "id": "NHpMl1b5iRFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(model1, input_size=(10, 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpizUdumhqnA",
        "outputId": "5a5e768d-fef2-4704-98d6-80ba97bd8a6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Model                                    [10, 1]                   --\n",
              "├─Linear: 1-1                            [10, 1]                   6\n",
              "├─Sigmoid: 1-2                           [10, 1]                   --\n",
              "==========================================================================================\n",
              "Total params: 6\n",
              "Trainable params: 6\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.00\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model 2**"
      ],
      "metadata": {
        "id": "I-VQ9X0yh-TJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries from PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a custom model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(num_features, 3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(3, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Define how input data flows through the layers\n",
        "    def forward(self, features):\n",
        "        out = self.layer1(features)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Create dummy input features (10 samples, each with 5 features)\n",
        "features = torch.rand(10, 5)\n",
        "\n",
        "# Instantiate the model with the number of input features\n",
        "model2 = Model(features.shape[1])"
      ],
      "metadata": {
        "id": "LMqNm9JZvLeT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a forward pass\n",
        "print(model2(features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upWBZUY_JPVc",
        "outputId": "c1e0cd51-29de-401b-db2b-bc3025334b30"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5436],\n",
            "        [0.5593],\n",
            "        [0.5552],\n",
            "        [0.5428],\n",
            "        [0.5418],\n",
            "        [0.5520],\n",
            "        [0.5438],\n",
            "        [0.5540],\n",
            "        [0.5513],\n",
            "        [0.5418]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show weights and bias of the first layer\n",
        "print(model2.layer1.weight)  # 5x3\n",
        "print(model2.layer1.bias)    # 3x1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrOABEg2JRkc",
        "outputId": "bae02562-1ee4-406e-d1f5-cee5d96bb0cb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.2688,  0.0239,  0.4432,  0.2677,  0.0229],\n",
            "        [-0.2138,  0.2217,  0.2949,  0.0895, -0.1404],\n",
            "        [-0.1854,  0.2483, -0.2754, -0.3980, -0.1566]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1358,  0.3508, -0.1073], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show weights and bias of the first layer\n",
        "print(model2.layer2.weight)  # 3x1\n",
        "print(model2.layer2.bias)    # 3x1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZBfVaAvJVtU",
        "outputId": "0d70c19e-48d3-4d5e-8eab-f143f23bcadd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.1254, -0.2584,  0.3098]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2839], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 2 summary**"
      ],
      "metadata": {
        "id": "psqueQN4iVaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model-2](https://raw.githubusercontent.com/mohd-faizy/PyTorch-Essentials/refs/heads/main/PyTorch_x1/_img/04_2.jpeg)"
      ],
      "metadata": {
        "id": "CgLSXFb4nfRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(model2, input_size=(10, 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cphFuGUzhr3B",
        "outputId": "8b40199f-2246-457e-d086-6f61dc12be3b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Model                                    [10, 1]                   --\n",
              "├─Linear: 1-1                            [10, 3]                   18\n",
              "├─ReLU: 1-2                              [10, 3]                   --\n",
              "├─Linear: 1-3                            [10, 1]                   4\n",
              "├─Sigmoid: 1-4                           [10, 1]                   --\n",
              "==========================================================================================\n",
              "Total params: 22\n",
              "Trainable params: 22\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.00\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ **Using Sequential Container**\n",
        "\n",
        ">It Allows you to stack multiple layers together in a modular and readable way"
      ],
      "metadata": {
        "id": "0gJvbvr7wQIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries from PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a custom model class inheriting from nn.Module\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()  # Initializes parent nn.Module class to enable parameter registration and other core functionality\n",
        "\n",
        "        # Define a simple feedforward neural network using nn.Sequential\n",
        "        # This block is assigned to self.model and includes:\n",
        "        # - Linear layer to reduce input features to 3 hidden units\n",
        "        # - ReLU activation for non-linearity\n",
        "        # - Linear layer mapping 3 hidden units to 1 output\n",
        "        # - Sigmoid activation to squash output between 0 and 1 (suitable for binary classification)\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(num_features, 3),  # num_features->5, 3 hidden units [not layers]\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(3, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    # Define how input data flows through the layers\n",
        "    def forward(self, features):\n",
        "        return self.network(features)\n",
        "\n",
        "# Create dummy input features (10 samples, each with 5 features)\n",
        "features = torch.rand(10, 5)\n",
        "\n",
        "# Instantiate the model with the number of input features\n",
        "model2 = Model(features.shape[1])\n",
        "\n",
        "# Perform a forward pass through the model\n",
        "print(model2(features))  # Output of the model (after passing through all layers)\n",
        "\n",
        "# Show weights and bias of the first linear layer inside the Sequential block\n",
        "print(model2.network[0].weight)  # Weights of first Linear layer\n",
        "print(model2.network[0].bias)    # Bias of first Linear layer"
      ],
      "metadata": {
        "id": "oJvxf-ftpdEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aef709f-ff70-4fc3-e696-3b4c6d929055"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4292],\n",
            "        [0.4209],\n",
            "        [0.4323],\n",
            "        [0.4353],\n",
            "        [0.4324],\n",
            "        [0.4338],\n",
            "        [0.4386],\n",
            "        [0.4246],\n",
            "        [0.4333],\n",
            "        [0.4228]], grad_fn=<SigmoidBackward0>)\n",
            "Parameter containing:\n",
            "tensor([[-0.4003, -0.1846, -0.0175, -0.4442,  0.2974],\n",
            "        [-0.0927,  0.1105,  0.0322,  0.1539, -0.1415],\n",
            "        [ 0.0625, -0.4223, -0.2045,  0.4421, -0.1345]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1412,  0.2851, -0.3888], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Optimizing**  [`03_PyTorch_Traininga_Pipeline`](https://drive.google.com/file/d/1Thw9YVUJPKRrzWPWdwGVQJrTbfnE3xMn/view?usp=sharing) **code**"
      ],
      "metadata": {
        "id": "PCaTXYRnzwkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Import Required Libraries ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# === Constants ===\n",
        "DATA_URL = \"https://raw.githubusercontent.com/mohd-faizy/PyTorch-Essentials/main/PyTorch_x1/_dataset/Breast-Cancer-Detection.csv\"\n",
        "DATA_FILE = \"Breast-Cancer-Detection.csv\"\n",
        "\n",
        "TEST_SIZE = 0.2\n",
        "SEED = 42\n",
        "LEARNING_RATE = 0.1\n",
        "EPOCHS = 100\n",
        "\n",
        "# === Download Dataset (use only in notebook) ===\n",
        "# If using in .py file, download manually\n",
        "import os\n",
        "if not os.path.exists(DATA_FILE):\n",
        "    import urllib.request\n",
        "    urllib.request.urlretrieve(DATA_URL, DATA_FILE)\n",
        "\n",
        "# === Function: Data Preprocessing Pipeline ===\n",
        "def load_and_preprocess_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.drop(columns=['id', 'Unnamed: 32'], inplace=True)\n",
        "\n",
        "    X = df.iloc[:, 1:].values\n",
        "    y = df.iloc[:, 0].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=SEED)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    y_train = encoder.fit_transform(y_train)\n",
        "    y_test = encoder.transform(y_test)\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor\n",
        "\n",
        "# === Load and Prepare Data ===\n",
        "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = load_and_preprocess_data(DATA_FILE)\n",
        "\n",
        "# === Neural Network Model ===\n",
        "class MySimpleNN(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(num_features, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, features):\n",
        "        out = self.layer1(features)\n",
        "        out = self.sigmoid(out)\n",
        "        return out  # ❗️Missing in original\n",
        "\n",
        "    def loss_function(self, y_pred, y):\n",
        "        epsilon = 1e-7\n",
        "        y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n",
        "        loss = -(y * torch.log(y_pred) + (1 - y) * torch.log(1 - y_pred)).mean()\n",
        "        return loss\n",
        "\n",
        "# === Initialize Model ===\n",
        "model = MySimpleNN(X_train_tensor.shape[1])\n",
        "\n",
        "# === Training Loop ===\n",
        "for epoch in range(EPOCHS):\n",
        "    y_pred = model(X_train_tensor)\n",
        "    loss = model.loss_function(y_pred, y_train_tensor)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.layer1.weight -= LEARNING_RATE * model.layer1.weight.grad\n",
        "        model.layer1.bias -= LEARNING_RATE * model.layer1.bias.grad\n",
        "\n",
        "    model.layer1.weight.grad.zero_()\n",
        "    model.layer1.bias.grad.zero_()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch + 1:03d} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "# === Model Evaluation ===\n",
        "with torch.no_grad():\n",
        "    y_pred_test = model(X_test_tensor)\n",
        "    y_pred_labels = (y_pred_test > 0.5).float()  # you can keep 0.9 if confident\n",
        "\n",
        "    accuracy = (y_pred_labels == y_test_tensor).float().mean()\n",
        "    print(f\"\\nTest Accuracy: {accuracy.item():.4f}\")"
      ],
      "metadata": {
        "id": "mZMvK_08LYug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51779a9c-9562-48f0-fdc7-c577d0025cbe"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 010 - Loss: 0.2559\n",
            "Epoch 020 - Loss: 0.1920\n",
            "Epoch 030 - Loss: 0.1630\n",
            "Epoch 040 - Loss: 0.1459\n",
            "Epoch 050 - Loss: 0.1344\n",
            "Epoch 060 - Loss: 0.1261\n",
            "Epoch 070 - Loss: 0.1196\n",
            "Epoch 080 - Loss: 0.1145\n",
            "Epoch 090 - Loss: 0.1102\n",
            "Epoch 100 - Loss: 0.1067\n",
            "\n",
            "Test Accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Optimised code\n",
        "\"\"\"\n",
        "\n",
        "# === Import Required Libraries ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# === Constants ===\n",
        "DATA_URL = \"https://raw.githubusercontent.com/mohd-faizy/PyTorch-Essentials/refs/heads/main/PyTorch_x1/_dataset/Breast-Cancer-Detection.csv\"\n",
        "DATA_FILE = \"Breast-Cancer-Detection.csv\"\n",
        "\n",
        "TEST_SIZE = 0.2\n",
        "SEED = 42\n",
        "LEARNING_RATE = 0.1\n",
        "EPOCHS = 100\n",
        "\n",
        "# === Download Dataset ===\n",
        "!wget -q -O {DATA_FILE} {DATA_URL}\n",
        "\n",
        "# === Data Preprocessing Function ===\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses the breast cancer dataset.\n",
        "\n",
        "    Steps:\n",
        "    - Drops irrelevant columns\n",
        "    - Encodes labels\n",
        "    - Standardizes features\n",
        "    - Converts data to PyTorch tensors\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.drop(columns=['id', 'Unnamed: 32'], inplace=True)\n",
        "\n",
        "    X = df.iloc[:, 1:].values\n",
        "    y = df.iloc[:, 0].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=SEED)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    y_train = encoder.fit_transform(y_train)\n",
        "    y_test = encoder.transform(y_test)\n",
        "\n",
        "    # Convert to float32 PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor\n",
        "\n",
        "# === Neural Network Definition ===\n",
        "class BreastCancerClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple feedforward neural network for binary classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(num_features, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# === Load and Prepare Data ===\n",
        "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = load_and_preprocess_data(DATA_FILE)\n",
        "\n",
        "# === Model Initialization and Loss Function ===\n",
        "model = BreastCancerClassifier(X_train_tensor.shape[1])\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)  # <--- Replaces manual SGD\n",
        "\n",
        "# === Training Loop ===\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    # Forward pass\n",
        "    y_pred = model(X_train_tensor)\n",
        "\n",
        "    # loss calculate\n",
        "    loss = loss_function(y_pred, y_train_tensor.view(-1, 1))\n",
        "\n",
        "    # clear gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Parameters update\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch + 10} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "# === Evaluation ===\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    predictions = (test_outputs > 0.5).float()\n",
        "    accuracy = (predictions == y_test_tensor).float().mean()\n",
        "    print(f\"\\nTest Accuracy: {accuracy.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUNyF2zCzwRb",
        "outputId": "40eae476-3569-4a3c-d06f-35d5428aacbe"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Loss: 0.6711\n",
            "Epoch 20 | Loss: 0.2466\n",
            "Epoch 30 | Loss: 0.1866\n",
            "Epoch 40 | Loss: 0.1596\n",
            "Epoch 50 | Loss: 0.1437\n",
            "Epoch 60 | Loss: 0.1329\n",
            "Epoch 70 | Loss: 0.1250\n",
            "Epoch 80 | Loss: 0.1188\n",
            "Epoch 90 | Loss: 0.1139\n",
            "Epoch 100 | Loss: 0.1098\n",
            "\n",
            "Test Accuracy: 0.9912\n"
          ]
        }
      ]
    }
  ]
}